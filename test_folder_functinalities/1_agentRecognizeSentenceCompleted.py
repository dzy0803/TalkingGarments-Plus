#!/usr/bin/env python3
import os, sys, queue, asyncio, tempfile, audioop
import sounddevice as sd
import webrtcvad
from pydub import AudioSegment
from dotenv import load_dotenv
import openai
import edge_tts

# === è¯»å– API KEY ===
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# === éŸ³é¢‘å‚æ•° ===
SAMPLE_RATE = 16000       # Whisperéœ€è¦16k
FRAME_DURATION = 30       # æ¯å¸§30ms
FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION / 1000)

# === VADé…ç½® ===
vad = webrtcvad.Vad()
vad.set_mode(2)  # 0-3ï¼Œè¶Šå¤§è¶Šæ•æ„Ÿ

# === å‚æ•°ï¼ˆå¯è°ƒï¼‰ ===
MIN_AUDIO_LEN = 0.5    # è‡³å°‘0.5ç§’æ‰ç®—æœ‰æ•ˆ
MAX_AUDIO_LEN = 20.0   # æœ€é•¿ä¸€å¥15ç§’
SILENCE_TIMEOUT = 0.8  # åœé¡¿1ç§’æ‰ç®—ç»“æŸ

# === é˜Ÿåˆ—ç¼“å­˜ ===
audio_queue = queue.Queue()

# === å¯¹è¯ä¸Šä¸‹æ–‡ ===
chat_history = [
    {
        "role": "system",
        "content": (
            "You are a friendly, natural, conversational AI assistant. "
            "Speak concisely, like a real person, and sound warm and engaging."
        )
    }
]

# === Whisper è¯­éŸ³è½¬æ–‡æœ¬ ===
def whisper_transcribe(pcm_bytes):
    duration = len(pcm_bytes) / 2 / SAMPLE_RATE
    if duration < MIN_AUDIO_LEN:
        print(f"âš ï¸ è·³è¿‡ <{MIN_AUDIO_LEN}s çš„éŸ³é¢‘")
        return ""

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        audio = AudioSegment(
            data=pcm_bytes,
            sample_width=2,
            frame_rate=SAMPLE_RATE,
            channels=1
        )
        audio.export(f.name, format="wav")

        try:
            with open(f.name, "rb") as audio_file:
                result = openai.Audio.transcribe("whisper-1", audio_file)
                return result["text"].strip()
        except openai.error.InvalidRequestError:
            print("âš ï¸ Whisperæ‹’ç»ï¼ŒéŸ³é¢‘å¤ªçŸ­/æ— æ•ˆ")
            return ""
        except Exception as e:
            print(f"âŒ Whisperå‡ºé”™: {e}")
            return ""

# === GPT-4o-mini å¯¹è¯ ===
def chat_with_gpt(user_text):
    chat_history.append({"role": "user", "content": user_text})
    trimmed = [chat_history[0]] + chat_history[-10:]
    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=trimmed,
        temperature=0.6
    )
    reply = resp.choices[0].message["content"]
    chat_history.append({"role": "assistant", "content": reply})
    return reply

# === Edge-TTS æœ—è¯» ===
async def speak_async(text):
    tts = edge_tts.Communicate(text, voice="en-US-JennyNeural")
    await tts.save("reply.mp3")
    os.system("mpg123 reply.mp3 > /dev/null 2>&1")

def speak(text):
    asyncio.run(speak_async(text))

# === éŸ³é¢‘å›è°ƒ ===
def audio_callback(indata, frames, time, status):
    if status:
        print("âš ï¸", status)
    audio_queue.put(bytes(indata))

# === è‡ªåŠ¨åˆ†å¥å½•éŸ³ï¼ˆåœé¡¿â‰¤1ç§’ä¸ç»“æŸï¼‰ ===
def vad_recording():
    print("ğŸ¤ Listeningâ€¦ you can pause â‰¤1s without ending the sentence.")

    with sd.RawInputStream(
        samplerate=SAMPLE_RATE,
        blocksize=FRAME_SIZE,
        dtype="int16",
        channels=1,
        callback=audio_callback
    ):
        pcm_buffer = b""
        speaking = False
        silence_time = 0.0

        while True:
            frame = audio_queue.get()
            if len(frame) < FRAME_SIZE * 2:
                continue

            is_speech = vad.is_speech(frame, SAMPLE_RATE)

            if is_speech:
                pcm_buffer += frame
                speaking = True
                silence_time = 0.0
            else:
                if speaking:
                    silence_time += FRAME_DURATION / 1000.0
                    if silence_time >= SILENCE_TIMEOUT:  # åœé¡¿è¶…è¿‡1ç§’
                        duration = len(pcm_buffer) / 2 / SAMPLE_RATE
                        if duration < MIN_AUDIO_LEN:
                            print(f"âš ï¸ å¤ªçŸ­({duration:.2f}s)ï¼Œä¸¢å¼ƒé‡å½•â€¦")
                            pcm_buffer = b""
                            speaking = False
                            silence_time = 0.0
                            continue

                        print(f"âœ… End of speech, length={duration:.2f}s")
                        return pcm_buffer

            # é˜²æ­¢ä¸€ç›´è¯´ > MAX_AUDIO_LEN ç§’
            duration = len(pcm_buffer) / 2 / SAMPLE_RATE
            if duration >= MAX_AUDIO_LEN:
                print(f"â³ è¶…è¿‡æœ€å¤§é•¿åº¦ {MAX_AUDIO_LEN}sï¼Œå¼ºåˆ¶ç»“æŸ")
                return pcm_buffer

# === ä¸»å¾ªç¯ï¼šè½®æµå¯¹è¯ ===
def run_conversation():
    while True:
        pcm_data = vad_recording()
        user_text = whisper_transcribe(pcm_data)
        if not user_text.strip():
            print("ğŸ¤” æ²¡å¬æ¸…ï¼Œå†è¯•ä¸€æ¬¡â€¦")
            continue

        print(f"ğŸ—£ï¸ User: {user_text}")
        reply = chat_with_gpt(user_text)
        print(f"ğŸ¤– Assistant: {reply}")
        speak(reply)

if __name__ == "__main__":
    print("ğŸ¯ LLM Agent ready. Start speaking, pause â‰¤1s wonâ€™t cut your sentence.")
    run_conversation()
