#!/usr/bin/env python3
import os, sys, queue, asyncio, tempfile, subprocess, threading
import sounddevice as sd
import webrtcvad
from pydub import AudioSegment
from dotenv import load_dotenv
import openai
import edge_tts
import time
import threading

# === è¯»å– API KEY ===
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# === éŸ³é¢‘å‚æ•° ===
SAMPLE_RATE = 16000       # Whisperéœ€è¦16k
FRAME_DURATION = 30       # æ¯å¸§30ms
FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION / 1000)

# === VADé…ç½® ===
vad = webrtcvad.Vad()
vad.set_mode(2)  # 0-3ï¼Œè¶Šå¤§è¶Šæ•æ„Ÿ

# === å‚æ•°ï¼ˆå¯è°ƒï¼‰ ===
MIN_AUDIO_LEN = 0.5    # è‡³å°‘0.5ç§’æ‰ç®—æœ‰æ•ˆ
MAX_AUDIO_LEN = 20.0   # æœ€é•¿ä¸€å¥20ç§’
SILENCE_TIMEOUT = 0.8  # åœé¡¿0.8ç§’æ‰ç®—ç»“æŸ

# === é˜Ÿåˆ—ç¼“å­˜ ===
audio_queue = queue.Queue()

# === å¯¹è¯ä¸Šä¸‹æ–‡ ===
chat_history = [
    {
        "role": "system",
        "content": (
            "You are a friendly, natural, conversational AI assistant. "
            "Speak concisely, like a real person, and sound warm and engaging."
        )
    }
]

# === å½“å‰æ’­æ”¾å™¨è¿›ç¨‹å¥æŸ„ï¼ˆç”¨äºæ‰“æ–­TTSï¼‰ ===
current_player = None
player_lock = threading.Lock()

# === æ‰“æ–­äº‹ä»¶ï¼ˆè®©ä¸»çº¿ç¨‹çŸ¥é“è¢«æ‰“æ–­ï¼‰ ===
interrupt_event = threading.Event()

# === Whisper è¯­éŸ³è½¬æ–‡æœ¬ ===
def whisper_transcribe(pcm_bytes):
    duration = len(pcm_bytes) / 2 / SAMPLE_RATE
    if duration < MIN_AUDIO_LEN:
        print(f"âš ï¸ è·³è¿‡ <{MIN_AUDIO_LEN}s çš„éŸ³é¢‘")
        return ""

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        audio = AudioSegment(
            data=pcm_bytes,
            sample_width=2,
            frame_rate=SAMPLE_RATE,
            channels=1
        )
        audio.export(f.name, format="wav")

        try:
            with open(f.name, "rb") as audio_file:
                result = openai.Audio.transcribe("whisper-1", audio_file)
                return result["text"].strip()
        except openai.error.InvalidRequestError:
            print("âš ï¸ Whisperæ‹’ç»ï¼ŒéŸ³é¢‘å¤ªçŸ­/æ— æ•ˆ")
            return ""
        except Exception as e:
            print(f"âŒ Whisperå‡ºé”™: {e}")
            return ""

# === GPT-4o-mini å¯¹è¯ ===
def chat_with_gpt(user_text):
    chat_history.append({"role": "user", "content": user_text})
    trimmed = [chat_history[0]] + chat_history[-10:]
    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=trimmed,
        temperature=0.6
    )
    reply = resp.choices[0].message["content"]
    chat_history.append({"role": "assistant", "content": reply})
    return reply

# === Edge-TTS æœ—è¯» ===
async def speak_async(text):
    tts = edge_tts.Communicate(text, voice="en-US-JennyNeural")
    await tts.save("reply.mp3")

def stop_speaking():
    """åœæ­¢å½“å‰TTSæ’­æ”¾"""
    global current_player
    with player_lock:
        if current_player and current_player.poll() is None:
            print("ğŸ›‘ åœæ­¢TTSæ’­æ”¾")
            current_player.terminate()
            current_player = None

def monitor_interrupt():
    """
    æ’­æ”¾TTSæ—¶ç›‘å¬ç”¨æˆ·è¯­éŸ³ï¼Œå¿…é¡»æ»¡è¶³ï¼š
      1. æ’­æ”¾0.5ç§’åæ‰å¼€å§‹æ£€æµ‹ï¼ˆé¿å…å›å£°ï¼‰
      2. å¿…é¡»è¿ç»­1.3ç§’çš„è¯­éŸ³å¸§ (~43å¸§) æ‰ç¡®è®¤çœŸäºº
      3. ç¬¬ä¸€æ¬¡æ£€æµ‹åä»éœ€åŒæ¬¡ç¡®è®¤ï¼Œé˜²æ­¢è¯¯è§¦
    """
    vad_for_interrupt = webrtcvad.Vad()
    vad_for_interrupt.set_mode(0)  # æœ€å®½æ¾ï¼Œé™ä½è¯¯åˆ¤
    
    speech_frames = 0
    needed_frames = 43  # 43å¸§â‰ˆ1.3ç§’è¿ç»­è¯­éŸ³æ‰ç®—æ‰“æ–­
    start_delay = 0.5   # å‰0.5så®Œå…¨ä¸ç›‘å¬ï¼Œé¿å…å›å£°
    start_time = time.time()
    
    first_detected = False
    confirm_timeout = 1.0  # ç¬¬ä¸€æ¬¡æ£€æµ‹å1ç§’å†…å¿…é¡»ç»§ç»­è¯´è¯æ‰èƒ½ç¡®è®¤
    
    with sd.RawInputStream(
        samplerate=SAMPLE_RATE,
        blocksize=FRAME_SIZE,
        dtype="int16",
        channels=1,
        callback=audio_callback
    ):
        while True:
            # æ’­æ”¾ç»“æŸå°±é€€å‡º
            with player_lock:
                if current_player is None or current_player.poll() is not None:
                    break  
            
            # æ’­æ”¾å¼€å§‹çš„0.5ç§’ç›´æ¥ä¸¢å¼ƒï¼Œé¿å…å›å£°è§¦å‘
            if time.time() - start_time < start_delay:
                try:
                    audio_queue.get(timeout=0.05)
                except queue.Empty:
                    pass
                continue
            
            # è·å–éŸ³é¢‘å¸§
            try:
                frame = audio_queue.get(timeout=0.05)
            except queue.Empty:
                continue
            
            # æ£€æµ‹å¸§æ˜¯å¦ä¸ºè¯­éŸ³
            if vad_for_interrupt.is_speech(frame, SAMPLE_RATE):
                speech_frames += 1
            else:
                speech_frames = 0  # ä¸­æ–­åé‡æ–°è®¡æ•°
            
            # ç¬¬ä¸€æ¬¡æ£€æµ‹åˆ°ç–‘ä¼¼è¯­éŸ³ â†’ ä¸ç«‹å³æ‰“æ–­ï¼Œè¿›å…¥ç¡®è®¤é˜¶æ®µ
            if speech_frames > 5 and not first_detected:
                first_detected = True
                first_time = time.time()
                print("ğŸ‘‚ æ£€æµ‹åˆ°ç–‘ä¼¼è¯­éŸ³ï¼Œç­‰å¾…ç¡®è®¤â€¦")
            
            # å¦‚æœç¬¬ä¸€æ¬¡æ£€æµ‹åï¼Œç”¨æˆ·æ²¡ç»§ç»­è¯´è¯ â†’ å–æ¶ˆæ£€æµ‹
            if first_detected:
                if time.time() - first_time > confirm_timeout and speech_frames < 5:
                    first_detected = False
                    speech_frames = 0
                    print("âŒ æ£€æµ‹å–æ¶ˆï¼šæ²¡æœ‰æŒç»­è¯­éŸ³")
            
            # æœ€ç»ˆç¡®è®¤ï¼šå¿…é¡»è¿ç»­1.3ç§’çš„è¯­éŸ³å¸§æ‰æ‰“æ–­
            if speech_frames >= needed_frames:
                print("âœ… ç¡®è®¤çœŸäººæŒç»­è¯´è¯ 1.3ç§’ â†’ åœæ­¢æ’­æ”¾å¹¶è¿›å…¥å½•éŸ³")
                stop_speaking()
                interrupt_event.set()
                break

def speak_and_listen(text):
    """æ’­æ”¾TTSï¼ŒåŒæ—¶ç›‘å¬ç”¨æˆ·æ˜¯å¦è¯´è¯æ‰“æ–­"""
    global current_player
    interrupt_event.clear()  # æ’­æ”¾å‰æ¸…ç†æ‰“æ–­æ ‡å¿—
    asyncio.run(speak_async(text))

    # ç”¨ Popen æ’­æ”¾ï¼Œå¯éšæ—¶ç»ˆæ­¢
    with player_lock:
        current_player = subprocess.Popen(
            ["mpg123", "-q", "reply.mp3"],  # -q é™éŸ³æ¨¡å¼
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )

    # å¼€å¯ç›‘å¬çº¿ç¨‹
    t = threading.Thread(target=monitor_interrupt, daemon=True)
    t.start()

    # ç­‰å¾…æ’­æ”¾å™¨ç»“æŸæˆ–è¢«æ‰“æ–­
    while True:
        if interrupt_event.is_set():
            print("ğŸ”„ è¯­éŸ³è¢«æ‰“æ–­ï¼Œæå‰é€€å‡ºæ’­æ”¾")
            break
        with player_lock:
            if current_player is None or current_player.poll() is not None:
                break
        time.sleep(0.05)

    # æ’­æ”¾ç»“æŸï¼Œæ¸…ç†çŠ¶æ€
    with player_lock:
        if current_player:
            current_player.wait()
            current_player = None

    return interrupt_event.is_set()  # è¿”å›æ˜¯å¦è¢«æ‰“æ–­

# === éŸ³é¢‘å›è°ƒ ===
def audio_callback(indata, frames, time, status):
    if status:
        print("âš ï¸", status)
    audio_queue.put(bytes(indata))

# === è‡ªåŠ¨åˆ†å¥å½•éŸ³ï¼ˆåœé¡¿â‰¤0.8sä¸ç»“æŸï¼‰ ===
def vad_recording():
    print("ğŸ¤ Listeningâ€¦ you can pause â‰¤0.8s without ending the sentence.")

    with sd.RawInputStream(
        samplerate=SAMPLE_RATE,
        blocksize=FRAME_SIZE,
        dtype="int16",
        channels=1,
        callback=audio_callback
    ):
        pcm_buffer = b""
        speaking = False
        silence_time = 0.0

        while True:
            frame = audio_queue.get()
            if len(frame) < FRAME_SIZE * 2:
                continue

            is_speech = vad.is_speech(frame, SAMPLE_RATE)

            if is_speech:
                pcm_buffer += frame
                speaking = True
                silence_time = 0.0
            else:
                if speaking:
                    silence_time += FRAME_DURATION / 1000.0
                    if silence_time >= SILENCE_TIMEOUT:  # åœé¡¿è¶…è¿‡0.8ç§’
                        duration = len(pcm_buffer) / 2 / SAMPLE_RATE
                        if duration < MIN_AUDIO_LEN:
                            print(f"âš ï¸ å¤ªçŸ­({duration:.2f}s)ï¼Œä¸¢å¼ƒé‡å½•â€¦")
                            pcm_buffer = b""
                            speaking = False
                            silence_time = 0.0
                            continue

                        print(f"âœ… End of speech, length={duration:.2f}s")
                        return pcm_buffer

            # é˜²æ­¢ä¸€ç›´è¯´ > MAX_AUDIO_LEN ç§’
            duration = len(pcm_buffer) / 2 / SAMPLE_RATE
            if duration >= MAX_AUDIO_LEN:
                print(f"â³ è¶…è¿‡æœ€å¤§é•¿åº¦ {MAX_AUDIO_LEN}sï¼Œå¼ºåˆ¶ç»“æŸ")
                return pcm_buffer

# === ä¸»å¾ªç¯ï¼šè½®æµå¯¹è¯ ===
def run_conversation():
    while True:
        pcm_data = vad_recording()
        user_text = whisper_transcribe(pcm_data)
        if not user_text.strip():
            print("ğŸ¤” æ²¡å¬æ¸…ï¼Œå†è¯•ä¸€æ¬¡â€¦")
            continue

        print(f"ğŸ—£ï¸ User: {user_text}")
        reply = chat_with_gpt(user_text)
        print(f"ğŸ¤– Assistant: {reply}")

        # æ–°çš„æ’­æ”¾å‡½æ•°ï¼Œæ”¯æŒç”¨æˆ·æ‰“æ–­
        interrupted = speak_and_listen(reply)

        # å¦‚æœç”¨æˆ·æ‰“æ–­äº†ï¼Œç«‹åˆ»è¿›å…¥å½•éŸ³ï¼Œä¸ç­‰TTSæ’­å®Œ
        if interrupted:
            print("ğŸ”„ è¢«æ‰“æ–­ â†’ ç«‹å³è¿›å…¥ä¸‹ä¸€è½®å½•éŸ³")
            continue

if __name__ == "__main__":
    print("ğŸ¯ LLM Agent ready. Speak anytime, you can INTERRUPT the assistant and it will listen immediately!")
    run_conversation()
